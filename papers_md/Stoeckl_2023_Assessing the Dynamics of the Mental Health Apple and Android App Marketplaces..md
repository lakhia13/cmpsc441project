# Stoeckl_2023_Assessing the Dynamics of the Mental Health Apple and Android App Marketplaces.

Journal of Technology in Behavioral Science 
https://doi.org/10.1007/s41347-023-00300-x

Assessing the Dynamics of the Mental Health Apple and Android App 
Marketplaces

S. E. Stoeckl1 

 · Edgardo Torres‑Hernandez2 

 · Erica Camacho1 

 · John Torous1 

Received: 29 April 2022 / Revised: 26 October 2022 / Accepted: 17 January 2023 
© The Author(s), under exclusive licence to Springer Nature Switzerland AG 2023

Abstract
Barriers to mental health care, including stigma, costs, and mental health professional shortages, have been exacerbated by 
the COVID-19 pandemic. Smartphone apps have the potential to increase scalability and improve access to mental health 
information, support, and interventions. However, evaluating these apps and selecting ones for use in care remain chal-
lenging, especially as apps are often updating and changing. Recommending apps requires knowledge of how stable apps 
are as the experience of one user several months ago may or may not be the same. A sample of 347 apps of the 650 apps 
on the M-health Index and Navigation Database (MIND) https:// minda pps. org were reviewed between September 1, 2021, 
and January 5, 2022. Apps were selected by time since their last review, with updates occurring on average approximately 
4 months from the last review. Eleven trained app evaluators reviewed apps across 105 evaluation criteria in 9 categories. 
Results were compared to initial ratings, identifying the changes that occurred. The average app updates every 433 days, 
though 19% were updated in the last 3 months and some nearly weekly. Changes in privacy and features made up the highest 
percentage of changes, both at 38%. The most frequently observed privacy-related change was increased privacy policy read-
ing level. Functionality parameters changed in 28% of apps. The most common functionality change was the removal of an 
accessibility feature. Clinical foundations changed in 18% of apps and 9% added supporting studies. Cost structure changed 
in 17% of apps, with 10% adding a fee for use of the app. Engagement features changed in 17% of the apps, with additions 
and removals of validated assessments or screeners most common. The dynamic nature of the app stores is reflected in app 
privacy, features, and functionality. These changes, reflected by the increased reading levels required to understand privacy 
policies, the decrease in accessibility features, and the additions of fees to access mobile apps, reflect the need to constantly 
review apps and understand how they are evolving. Patient and clinicians should use the most recent and updated possible 
when evaluating apps.

Keywords  Mental health · Smartphones · Apps · mHealth · App evaluation, App rating, Mobile app privacy

Introduction

Approximately 1 in 5 US adults experienced mental illness 
in 2020, yet only 46% received treatment (Substance Abuse 
& Mental Health Services Administration, 2021). Barriers 
to treatment include stigma, privacy concerns, costs, time 
constraints, and mental health professional shortages (Lattie 
et al., 2022; Mental Health Care Health Professional Shortage 

 *  John Torous 

jtorous@bidmc.harvard.edu

1  Division of Digital Psychiatry, Beth Israel Deaconess 

Medical Center, Harvard Medical School, 330 Brookline 
Ave, Boston, MA 02446, USA

2  San Juan Bautista School of Medicine, Caguas, PR, USA

Areas, 2021). The COVID-19 pandemic introduced new bar-
riers and exacerbated existing ones. While the demand for 
mental health services has increased, accessibility to face-to-
face care has decreased (National Center for Health Statistics, 
2020; Ganson et al., 2020; Pierce et al., 2021; Purtle, 2020; 
World Health Organization, 2020). Underserved and high-
risk populations continue to be disproportionately affected 
(National Center for Health Statistics, 2020; Kirby, 2020; 
Nagata et al., 2021). The growing demand for services has 
increased the need for unique solutions.

Mobile technology and smartphone apps offer one such 
solution  with  the  potential  to  increase  the  capacity  and 
improve  access  to  mental  health  information,  support, 
and interventions through scalable smartphones (Figueroa 
&  Agilera,  2020).  In  2020,  85%  of  Americans  owned  a 

Vol.:(0123456789)1 3 
smartphone, and access was high across all demographics, 
including minority and disadvantaged populations (Pew 
Research Center, 2021). This high prevalence of smart-
phone ownership suggests that mobile health interventions 
have the potential to improve access and reach traditionally 
underserved populations. Interest in mobile health solutions 
has grown since their introduction over 10 years ago, and 
the COVID-19 pandemic amplified interest to new high lev-
els for all stakeholders, including patients, providers, and 
health insurance payers (Inkster & Digital Mental Health 
Data Insights Group, 2021).

Given the pressing need for more mental health services, 
research and commercial offerings have rapidly expanded. 
The literature indicates that apps are highly acceptable to 
patients, even those with serious mental illness (Torous 
et al., 2018), and feasibility is no longer a question. Results 
are more mixed for efficacy, with the most robust data sug-
gesting a small but positive effect that is often less than that 
marketed (Goldberg et al., 2022). However, even the most 
comprehensive literature reviews do not identify which apps 
may be most effective for any particular patient or how to 
make an informed choice regarding selection.

The challenge is pressing, with even pre-COVID data 
suggesting that over 30% of youth may already be turning 
to mental health apps as first-line tools when they have a 
mental health concern (Cohen et al., 2021). While more 
than 10,000 mental health focused apps are available, few 
are empirically supported. An analysis of 278 mental health 
apps revealed that only 16% were supported by a feasibility 
or efficacy study (Lagan et al., 2021a). In addition, transla-
tion and implementation from clinical to real-world settings 
have been a challenge (Lattie et al., 2022; Lipschitz et al., 
2019; Torous et al., 2018). Moreover, the fact that apps are 
dynamic and constantly updated makes evaluating them even 
more challenging for clinicians and patients. A 2016 paper 
examined the dynamics of mental health apps and reported 
that every 2.9 days a depression app disappears from the 
app stores (Larsen et al., 2016). However, this paper only 
explored the rate of apps being removed from the market-
place and not the nature of changes within apps over time.

In addition to the clear clinical decision-making benefits 
of understanding apps changes over time, this knowledge is 
also relevant to regulatory efforts. The FDA’s next genera-
tion of regulation of software as a medical device is called 
the Pre-Certification program. A critical component of the 
program is to use real-world data to evaluate when an app 
needs to be re-examined or re-approved (Kadakia et al., 
2020). Data on how apps change over time is thus neces-
sary to inform programs like Pre-Certification.

The majority of apps that fall outside of this regulation 
instead self-identified as wellness devices (Kahane et al., 
2021). This information is still critical for patients and clini-
cians seeking to evaluate the stability of wellness apps. Given 

Journal of Technology in Behavioral Science

that most wellness apps are not bound to protect personal 
health data and can legally transfer or sell it to third parties 
(Blenner et al., 2016), the need for timely information about 
risks of these apps is important. Recognition of this need has 
led professional organizations like the American Psychiatric 
Association (APA) to offer public facing guidance and educa-
tion around evaluation of apps (Lagan et al., 2021b).

The APA’s App Evaluation Model was developed as a 
way to critically evaluate an app by answering questions 
regarding background information, privacy and security, 
clinical foundation, ease of use, and interoperability. It was 
developed through a six-step process distilling 961 ques-
tions across 45 app evaluation frameworks into 357 ques-
tions across five categories, then updated for clarity in con-
cert with the APA’s Expert App Evaluation Panel (Henson 
et al., 2019). The model’s interrater reliability is detailed 
in previous literature (Lagan et al., 2020). The APA App 
Evaluation Model was operationalized into 105 objective 
binary or numeric questions, which serve as the foundation 
of the M-health Index and Navigation Database (MIND) (see 
Appendix) (Lagan et al, 2020).

The M-health Index and Navigation Database (MIND), 
accessible at https:// minda pps. org, is an easily accessible 
and free database of over 650 mental health apps, each rated 
across 105 questions with a focus on accessibility, privacy, 
evidence, engagement, and therapeutic goal. MIND has 
been noted as one of the most comprehensive databases that 
reflects diversity, equity, and inclusion principles (Ramos 
et al., 2021), adopted by third parties (Spadaro et al., 2022), 
and reaches beyond academics into the popular press (The 
Economist, 2021; Esquire, 2021; Maldarelli & Whitcomb, 
2022).  Users  are  able  to  filter  by  criteria  and  find  apps 
that meet their individual needs and access app informa-
tion across nine categories. Apps on MIND are regularly 
updated within 100–180 days to ensure current information 
and provide insights over time. Thus, the detailed data on 
and frequent updating of apps on mindapps.org presents an 
ideal platform to study the dynamic of apps and how they 
change over time. This paper analyzes apps available on 
MIND to investigate prominent changes in efficacy, safety, 
and privacy over time.

Methods

A sample of 347 apps of the 650 apps available on MIND 
were reviewed between September 1, 2021, and January 5, 
2022. Apps were selected by time since their last review 
with the oldest apps rated first. All apps were updated 
within 6 months of last review, with updates occurring 
on average approximately 4 months from the last review. 
Apps  were  downloaded  from  the  Apple  iOS  store  and 
Google  Play  store  and  reviewed  by  eleven  trained  app 

1 3 
Journal of Technology in Behavioral Science 

evaluators by updating answers to 105 questions in 9 cate-
gories, according to the American Psychiatric Association 
App Evaluation Model (American Psychiatric Association, 
2022). All raters underwent interrater reliability training. 
Training included instruction on the APA App Evaluation 
Model, reviewing each of the 105 questions and poten-
tial responses with the project manager, practicing app 
evaluations and reviewing potential errors, and passing 
an assessment of interrater reliability. Interrater reliability 
was assessed using Cohen’s kappa statistic (Lagan et al., 
2020), for which raters demonstrated very good interrater 
reliability, defined as a kappa value above 0.750. App eval-
uations were reviewed by the project manager and raters 
met with the project manager weekly to review questions. 
After app evaluation of 347 apps, data was tabulated and 
change parameters were measured by percentage of apps 
changed across the following 9 categories: privacy, fea-
tures, functionalities, cost, clinical foundations, engage-
ments, supported conditions, uses, and developer type. See 
Appendix for a complete list of evaluation criteria.

Results

The  average  app  updates  every  433  days,  though  19% 
(n = 65) were updated in the last 3 months and some nearly 
weekly. In the 3-month period between September 1, 2021, 
and January 5, 2022, 6% (n = 21) of apps were removed 
from platforms. The percentage of apps with changes was 
high across the nine app rating categories. Application 
changes were at least 10% in 8 of 9 categories, 27% in 3 
of 9 categories, and 38% in 2 of 9 categories. Only the 
category of developer type showed little change, which is 
to be expected. Details are shown below in Table 1.

Table 1   App  changes  by  category,  September  1,  2021–January  5, 
2022

Category

Privacy & security
Features
Functionality
Evidence & clinical foundations
Cost
Engagements
Supported condition
Uses
Developer type

% apps 
changed by 
category

37.73%
37.62%
27.76%
18.41%
17.43%
17.15%
12.23%
11.00%
1.53%

Privacy and Security

Privacy and security factors changed in 38% (n = 133) of 
apps. The most frequently observed privacy-related change 
was increased privacy policy reading level. The next most 
common change was the addition of the ability to delete 
your own data.

Features

Features also changed in 38% (n = 133) of apps. The four 
most common changes to app features were as follows: the 
addition of physical exercise, addition of goal setting related 
content, removal of productivity, and removal of psychoe-
ducation content.

Functionality

Functionality parameters changed in 28% (n = 88) of apps. 
The most common change was the removal of an accessi-
bility feature. The subsequent most common changes were 
additions to exportability and offline access. Functionality 
features were removed (53%) more often than they were 
added (47%).

Clinical Foundation

Clinical foundations changed in 18% (n = 58) of the apps, 
with the most common changes being improvements or addi-
tions (84%). The most common changes indicate that apps 
have improved the writing and correctness of content and 
added disclaimers that they are not a replacement for care. 
In addition, 9% (n = 29) of apps added supporting studies.

Table 2   Most common app changes by evaluation criteria, September 
1, 2021–January 5, 2022

Category

Evaluation criteria

% apps 
changed by 
criteria

Features

Privacy & security Increased reading level
6%
Added can delete data
5%
Removed data stored on device
4%
3%
Added goal setting/habits
Added physical exercise tracking 3%
3%
Removed productivity
3%
Removed psychoeducation
5%
Removed accessibility feature
3%
Added exportability
3%
Added offline accessibility

Functionality

1 3Table 3   Changes by category in paid and unpaid apps with the greater 
percentage of change in each caterogy presented in bold

Category

Unpaid
% apps changed

Paid
% apps chanaged

Features
Privacy & security
Functionality
Cost
Clinical foundations
Engagements
Supported condition
Uses

33.01%
33.33%

25.89%
15.21%
15.21%

14.89%
12.30%
9.71%

26.67%

40.00%
16.67%

20.00%
16.67%
13.33%
3.33%
3.33%

Engagement Style, Cost Structure, and Supported 
Conditions

Engagement features changed in 17% (n = 54) of the apps, 
and of those changes 52% were additions and 48% were 
removals. The majority of the changes were additions and 
removals  of  clinically  validated  assessments  or  screen-
ers. Cost structure changed in 17% (n = 56) of apps, with 
10% adding a fee for use of the app. Supported conditions 
changed in 12% (n = 40) of apps. Mood disorders and sleep 
were the most common conditions added, while stress and 
anxiety conditions were the most commonly removed.

Comparison of Paid and Unpaid Apps

Although 35% (n = 114) of apps were totally free and 66% 
(n = 216) were free to download, 41% (n = 143) offered in-
app purchases, 35% (n = 114) offered subscriptions, and 20% 
(n = 70) had a one-time payment. Of free to download apps, 
88% (n = 190) also offered in-app purchases, subscriptions, 
or one-time payment. Paid apps (apps that were not totally 
free or free to download) changed more than unpaid apps in 

Journal of Technology in Behavioral Science

privacy, cost, and clinical foundations. Unpaid apps changed 
more than paid apps in features, functionalities, engage-
ments, supported conditions, and uses as shown in Tables 2 
and 3. There were technical issues in 20% of unpaid apps, 
but only 3% of paid apps.

Comparison of Developer Types

Eighty-eight  percent  (n = 304)  of  apps  reviewed  in  this 
paper from MIND were from for-profit developers. Aca-
demic institutions developed 3% (n = 10) of apps, govern-
ment 4% (n = 14), nonprofit companies 3% (n = 10), and 
healthcare providers 3% (n = 9). Partnerships between two 
or more developer types were responsible for 4% (n = 14) 
of apps. The app rating categories with the highest changes 
(see Fig. 1) were not uniform across app developer types, 
but privacy was consistently high across developer types. 
Thirty-three percent (n = 3) of apps from healthcare devel-
opers made privacy and clinical foundations changes; 21% 
(n = 3) of apps from government developers made privacy, 
features, functionality, clinical foundations, and engage-
ment changes; 50% (n = 5) of apps from non-profit develop-
ers made changes to privacy; and 70% (n = 7) of apps from 
academic developers made changes in clinical foundations.

Discussion

Our results confirm the dynamic nature of mental health 
apps and highlight frequent changes in their privacy protec-
tions, functionality, and features. While how these changes 
may benefit or harm the user is not simple to discern, we 
can say that the frequent and varied nature of these changes 
necessitates evaluation based on constantly updated data.

Dynamic pricing models combined with transparency in 
data reporting from apps will be necessary to understand 

Fig. 1   % Apps changed in APA evaluation categories, by developer type

1 3 
Journal of Technology in Behavioral Science 

the  economic  value  of  mental  health  apps.  For  health 
insurance payers to offer app formularies, they need to 
create metrics to assess these changes in the app landscape 
or tightly control app updates themselves. Other aspects 
necessary for valuation, such as real-world engagement 
with the apps and mean effect size, are not publicly avail-
able and thus not reported in our results.

One practical consideration of our results is to caution 
patients and clinicians away from static app rating systems 
that rely on outdated information. While having lists of 
“the best” or “top 10” apps is appealing, the challenge in 
trusting data from these sources is highlighted with our 
results showing that after just 4 months over a quarter of 
apps made changes to privacy, features, or functionality. 
While many app rating websites or services do not always 
note when they last updated content, a 2019 paper offers 
information with Mindtool.io having a mean of 714 days, 
Psyberguide 598 days, and ORCHA 109 days (Carlo et al., 
2019). Our team’s prior findings that apps not updated 
in 180 days may be a marker for lower quality apps (Liu 
et al., 2019) offer further caution towards relying on out-
of-date apps in addition to reviews of apps that are also 
out of date.

Our findings also suggest that mental health apps are 
becoming more difficult for the average user to navigate. 
This is depicted by the increased reading levels required 
to understand privacy policies, the decrease in accessibility 
features, and the additions of fees to access mobile apps. 
The most common changes to privacy, functionality, and 
cost have profound implications, especially for vulnerable or 
high-risk populations. Privacy risk is high—a great percent-
age of the apps reviewed made changes to privacy features, 
yet users are unlikely to review privacy feature changes regu-
larly. Those who regularly review privacy features will find 
information on data sharing challenging to parse, with miss-
ing or complicated explanations around sharing protected 
health information, de-identified data, anonymized data, 
and sharing policies with third parties. One solution to the 
increasing complexity of app evaluation is to offer support in 
the form of a trained staff member, called a digital navigator 
(Wisniewski et al., 2020), who can guide users through the 
many choices. Researchers examining app evaluation have 
noted that the process can feel like a “minefield” (Szinay 
et al., 2021), called for assistance from digital navigators 
(Roberts et al., 2021), and offered detailed training resources 
(Wisniewski et al., 2020).

Results  of  our  study  are  in  line  with  prior  work. 
Larsen et al. in 2016 found that 50% of marketplace apps 
were no longer found using the same search terms after 
130 days but did not examine how many apps were actu-
ally removed from the marketplace. We found nearly 40% 
change in apps across a similar time frame although did 
not confirm if these changes resulted in different search 

results. When updating the 177 apps reviewed on Pys-
berguide, the authors report that 9% of those apps they 
publicly reviewed were no longer available (Neary et al., 
2021), which is similar to our result of 6%.

This study is limited to apps in MIND and the sample 
may not be representative as app update order was not 
randomized. Important metrics such as average length of 
user engagement (Molloy & Anderson, 2021) and average 
effect size of an app are not publicly available and thus 
we are not able to code them on MIND or report on them 
around app dynamics.

Conclusion

This study highlights the dynamic nature of the app store 
environments,  revealing  rapid  and  substantial  changes 
that could present challenges for app selection, consumer 
safety, and assessing the economic value of apps. Future 
studies could evaluate the dynamics of particular subsets 
of apps in MIND, focusing on app types or conditions. 
Follow-up studies could investigate the reasons for app 
changes, for example, reasons privacy policy language 
reading levels are increasing or reasons for removal of 
apps from the marketplaces. There may also be changes in 
the app landscape dynamics pre- and post-pandemic due 
to COVID-19’s impact on mental health apps. With time, 
researchers using mindapps.org will be able to gather data 
over longer and longer periods, potentially allowing for 
modeling changes in the app landscape.

Appendix

Appendix. MindApps Database: complete list of evalua-
tion categories and criteria

Category

Platform

Cost

Developer types

Criteria

Android
iOs
Web
Free to download
Totally free
One-time payment
In-app purchase(s)
Subscription
Government
For profit company

1 3Category

Criteria

Category

Criteria

Journal of Technology in Behavioral Science

Supported conditions

Functionalities

Uses

Features

Non-profit company
Healthcare company
Academic institution
Mood disorders
Stress & anxiety
Sleep
Phobias
OCD
Schizophrenia
Eating disorders
Personality disorders
Self-harm
PTSD
Substance use
Substance use (alcohol)
Substance use (smoking & 

tobacco)
Headache
Pain
Non-specific
Spanish
Offline
Accessibility
Own your own data
Email or export your data
Send your data to a medical record
Self help
Reference
Hybrid
Track mood
Track medication
Track sleep
Track symptoms
Productivity
Physical health
Psychoeducation
Journaling
Mindfulness
Deep breathing
Picture gallery/hope board
iCBT or sleep therapy
CBT
ACT 
DBT
Peer support
Coach/therapist connection
Biodata
Goal setting/habits
Physical health exercises
Chatbot interaction
Bio feedback with sense data

Engagements

Application inputs

User generated data
Chat/message
Assessments/screenings
Real time response
Asynchronous response
Gamification (points/badges)
Videos
Audio/music/scripts
AI support
Peer support
Network support
Collaborative with provider/other
Surveys
Diary
Geolocation
Camera
Microphone
Step count
External devices
Social network
Notifications
References/information
Social network
Reminders
Graphs of data
Summary of data
Link to formal care/coaching
Evidence & clinical foundations Well-written relevant content

Application outputs

Privacy & security

Does what it claims
Patient facing
Can cause harm
Use warning
Supporting studies
Has privacy policy
Data stored on device
Data stored on server
Can delete data
App declares data use and purpose
App reports security measures in 

place

Is PHI shared
Is de-identified data shared
Is anonymized/aggregate data 

shared

Can opt out of data collection
Claims to meet HIPAA
Has crisis management feature
Reading grade level of privacy 

policy

1 3 
Journal of Technology in Behavioral Science 

Author Contribution  Conception/model: JT, EC. Data collection: SES, 
ETH, EC. Analysis/interpretation: SES, EC, JT. Writing: SES, EC, 
ETH, JT. Editing: SES, EC, ETH, JT.

Funding  Argosy Foundation.

Data Availability  Updated data is avalaible on minda pps. org.

Declarations 

Competing  Interests  JT  has  received  past  research  support  from 
Otsuka. JT is the co-founder of a mental health company called Pre-
cision Mental Wellness. SES is an employee of Happify Health and 
consultant for Found Health. These apps were not rated on MINDapps 
by SES to ensure impartiality.

References

American Psychiatric Association. (2022). The app evaluation model. 
Retrieved February 25, 2022, from https:// www. psych iatry. org/ 
psych iatri sts/ pract ice/ mental- health- apps/ app- evalu ation- model

Blenner, S. R., Köllmer, M., Rouse, A. J., Daneshvar, N., Williams, 
C., & Andrews, L. B. (2016). Privacy policies of Android dia-
betes apps and sharing of health information. JAMA, 315(10), 
1051–1052. https:// doi. org/ 10. 1001/ jama. 2015. 19426

Carlo, A. D., Hosseini Ghomi, R., Renn, B. N., & Areán, P. A. (2019). 
By the numbers: Ratings and utilization of behavioral health 
mobile applications. NPJ Digital Medicine, 2, 54. https:// doi. org/ 
10. 1038/ s41746- 019- 0129-6

Cohen, K. A., Stiles-Shields, C., Winquist, N., & Lattie, E. G. (2021). 
Traditional and nontraditional mental healthcare services: Usage 
and preferences among adolescents and younger adults. The Jour-
nal of Behavioral Health Services & Research, 20, 1–7. https:// 
doi. org/ 10. 1007/ s11414- 020- 09746-w

Dramatic growth in mental-health apps has created a risky industry. 
(2021, December 11). The economist. https:// www. econo mist.  
com/ busin ess/ 2021/ 12/ 11/ drama tic- growth- in- mental- health-  
apps- has- creat ed-a- risky- indus try

Figueroa, C. A., & Aguilera, A. (2020). The need for a mental health 
technology revolution in the COVID-19 pandemic. Frontiers in 
Psychiatry, 11, 523. https:// doi. org/ 10. 3389/ fpsyt. 2020. 00523
Ganson,  K.  T.,  Weiser,  S.  D.,  Tsai,  A.  C.,  et  al.  (2020).  Associa-
tions  between  anxiety  and  depression  symptoms  and  medi-
cal  care  avoidance  during  COVID-19.  Journal  of  General 
Internal  Medicine,  35,  3406–34082. https:// doi. org/ 10. 1007/  
s11606- 020- 06156-8

Goldberg, S. B., Lam, S. U., Simonsson, O., Torous, J., & Sun, S. 
(2022). Mobile phone-based interventions for mental health: A 
systematic meta-review of 14 meta-analyses of randomized con-
trolled trials. PLOS Digital Health, 1(1), e0000002. https:// doi. 
org/ 10. 1371/ journ al. pdig. 00000 02

Henson, P., David, G., Albright, K., & Torous, J. (2019). Deriving a 
practical framework for the evaluation of health apps. The Lancet 
Digital Health, 1(2), e52–e54.

Inkster, B., & Digital Mental Health Data Insights Group (DMHDIG). 
(2021). Early warning signs of a mental health tsunami: a coordi-
nated response to gather initial data insights from multiple digital 
services providers. Frontiers in digital health, 2, 578902. https:// 
doi. org/ 10. 3389/ fdgth. 2020. 578902

Kadakia, K., Patel, B., & Shah, A. (2020). Advancing digital health: 
FDA innovation during COVID-19. NPJ Digital Medicine, 3(1), 
161. https:// doi. org/ 10. 1038/ s41746- 020- 00371-7

Kahane, K., François, J., & Torous, J. (2021). Perspective: The digital 
health app policy landscape: Regulatory gaps and choices through 
the lens of mental health. The Journal of Mental Health Policy 
and Economics, 24(3), 101–108. PMID: 34554108.

Kirby, T. (2020). Evidence mounts on the disproportionate effect of 
COVID-19 on ethnic minorities. The Lancet Respiratory Medi-
cine,  8(6),  547–548. https:// doi. org/ 10. 1016/ s2213- 2600(20)  
30228-9

Lagan, S., Aquino, P., Emerson, M. R., Fortuna, K., Walker, R., & 
Torous, J. (2020). Actionable health app evaluation: Translating 
expert frameworks into objective metrics. npj Digital Medicine, 
3(1). https:// doi. org/ 10. 1038/ s41746- 020- 00312-4

Lagan, S., D’Mello, R., Vaidyam, A., Bilden, R., & Torous, J. (2021a). 
Assessing mental health apps marketplaces with objective metrics 
from 29,190 data points from 278 apps. Acta Psychiatrica Scan-
dinavica, 144(2), 201–210. https:// doi. org/ 10. 1111/ acps. 13306
Lagan, S., Emerson, M. R., King, D., Matwin, S., Chan, S. R., Proctor, S., 
Tartaglia, J., Fortuna, K. L., Aquino, P., Walker, R., Dirst, M., Benson, 
N., Myrick, K. J., Tatro, N., Gratzer, D., & Torous, J. (2021b). Mental 
health app evaluation: Updating the American Psychiatric Associa-
tion’s Framework Through a Stakeholder-Engaged Workshop. Psychi-
atric services (Washington, D.C.), 72(9), 1095–1098. https:// doi. org/ 
10. 1176/ appi. ps. 20200 0663

Larsen, M. E., Nicholas, J., & Christensen, H. (2016). Quantifying 
app store dynamics: Longitudinal tracking of mental health apps. 
JMIR mHealth and uHealth, 4(3), e96. https:// doi. org/ 10. 2196/  
mheal th. 6020

Lattie, E. G., Stiles-Shields, C., & Graham, A. K. (2022). An over-
view of and recommendations for more accessible digital mental 
health services. Nature Reviews Psychology. https:// doi. org/ 10. 
1038/ s44159- 021- 00003-1

Lipschitz, J., Hogan, T. P., Bauer, M. S., & Mohr, D. C. (2019). Closing 
the research-to-practice gap in digital psychiatry. The Journal of 
Clinical Psychiatry, 80(3). https:// doi. org/ 10. 4088/ jcp. 18com 12659
Liu, G., Henson, P., Keshavan, M., Pekka-Onnela, J., & Torous, J. 
(2019). Assessing the potential of longitudinal smartphone based 
cognitive assessment in schizophrenia: A naturalistic pilot study. 
Schizophrenia research. Cognition, 17, 100144. https:// doi. org/  
10. 1016/j. scog. 2019. 100144

Maldarelli, C., & Whitcomb, I. (2022). Mental wellness apps are 
basically the Wild West of therapy. Popular Science. Retrieved 
February  1,  2022,  from  https:// www. popsci. com/ scien ce/ 
mental- health- apps- safety/

Mental Health Care Health Professional Shortage Areas (HPSAs). 
(2021). Kaiser Family Foundation. Retrieved February 1, 2022, 
from https:// www. kff. org/ other/ state- indic ator/ mental- health- care- 
health- profe ssion al- short age- areas- hpsas/? curre ntTim eframe= 0& 
sortM odel=% 7B% 22col Id% 22:% 22Loc ation% 22,% 22sort% 22:% 
22asc% 22% 7D

Molloy, A., & Anderson, P. L. (2021). Engagement with mobile health 
interventions for depression: A systematic review. Internet inter-
ventions,  26,  100454.  https:// doi. org/ 10. 1016/j. invent. 2021. 
100454

Nagata, J. M., Ganson, K. T., Bonin, S. L., Twadell, K. L., Garcia, M. 
E., Langrock, O., Vittinghoff, E., Tsai, A. C., Weiser, S. D., & 
Abdel Magid, H. S. (2021). Prevalence and sociodemographic 
correlates of unmet need for mental health counseling among 
adults during the COVID-19 pandemic. Psychiatric Services, 
appi.ps.2021001. https:// doi. org/ 10. 1176/ appi. ps. 20210 0111
National Center for Health Statistics. (2020). Mental health: Household 
pulse survey. US Centers for Disease Control and Prevention. 
Retrieved December 27, 2021, from https:// www. cdc. gov/ nchs/ 
covid 19/ pulse/ mental- health. htm

Neary, M., Bunyi, J., Palomares, K., Mohr, D. C., Powell, A., Ruzek, 
J., Williams, L. M., Wykes, T., & Schueller, S. M. (2021). A 
process for reviewing mental health apps: Using the One Mind 

1 3PsyberGuide Credibility Rating System. DIGITAL HEALTH, 7, 
205520762110536. https:// doi. org/ 10. 1177/ 20552 07621 10536 90
Pew Research Center. (2021). Mobile fact sheet. Pew Research Center: 
Internet, Science & Tech. Retrieved January 1, 2022, from https:// 
www. pewre search. org/ inter net/ fact- sheet/ mobile

Pierce, B. S., Perrin, P. B., Tyler, C. M., et al. (2021). The COVID-19 
telepsychology revolution: A national study of pandemic-based 
changes in US mental health care delivery. American Psycholo-
gist, 2021(76), 14–253. https:// doi. org/ 10. 1037/ amp00 00722
Purtle, J. (2020). COVID-19 and mental health equity in the United 
States. Social Psychiatry and Psychiatric Epidemiology, 2020(55), 
969–971. https:// doi. org/ 10. 1007/ s00127- 020- 01896-8

Ramos, G., Ponting, C., Labao, J. P., & Sobowale, K. (2021). Consider-
ations of diversity, equity, and inclusion in mental health apps: A 
scoping review of evaluation frameworks. Behaviour research and 
therapy, 147, 103990. https:// doi. org/ 10. 1016/j. brat. 2021. 103990
Roberts, A. E., Davenport, T. A., Wong, T., Moon, H. - W., Hickie, I. 
B., & Lamonica, H. M. (2021). Evaluating the quality and safety 
of health-related apps and e-tools: Adapting the Mobile App Rat-
ing Scale and developing a quality assurance protocol. Internet 
Interventions, 24, 100379. https:// doi. org/ 10. 1016/j. invent. 2021. 
100379

Substance Abuse and Mental Health Services Administration. (2021). 
Key substance use and mental health indicators in the United 
States: Results from the 2020 National Survey on Drug Use and 
Health (HHS Publication No. PEP21–07–01–003, NSDUH Series 
H-56). Rockville, MD: Center for Behavioral Health Statistics and 
Quality, Substance Abuse and Mental Health Services Adminis-
tration. Retrieved December 27, 2021, from https:// www. samhsa. 
gov/ data

Spadaro,  B.,  Martin-Key,  N.  A.,  Funnell,  E.,  &  Bahn,  S.  (2022). 
mHealth solutions for perinatal mental health: Scoping review and 

Journal of Technology in Behavioral Science

appraisal following the mHealth index and navigation database 
framework. JMIR mHealth and uHealth, 10(1), e30724. https:// 
doi. org/ 10. 2196/ 30724

Szinay, D., Perski, O., Jones, A., Chadborn, T., Brown, J., & Naughton, 
F. (2021). Influences on the uptake of health and well-being apps 
and curated app portals: Think-aloud and interview study. JMIR 
mHealth  and  uHealth,  9(4),  e27173.  https:// doi. org/ 10. 2196/  
27173

The reluctant man’s guide to starting therapy. (2021, December 30). 
Esquire. https:// www. esqui re. com/ lifes tyle/ a3824 1141/ guide- to- 
start ing- thera py- mental- health/

Torous, J., Wisniewski, H., Liu, G., & Keshavan, M. (2018). Mental 
health mobile phone app usage, concerns, and benefits among 
psychiatric outpatients: Comparative survey study. JMIR Mental 
Health, 5(4), e11715. https:// doi. org/ 10. 2196/ 11715

Wisniewski,  H.,  Gorrindo,  T.,  Rauseo-Ricupero,  N.,  Hilty,  D.,  & 
Torous, J. (2020). The role of digital navigators in promoting 
clinical care and technology integration into practice. Digital Bio-
markers, 4(Suppl 1), 119–135. https:// doi. org/ 10. 1159/ 00051 0144
World Health Organization. (2020). The impact of COVID-19 on men-
tal, neurological and substance use services: Results of a rapid 
assessment. Retrieved December 27, 2021 from https:// apps. who. 
int/ iris/ bitst ream/ handle/ 10665/ 335838/ 97892 40012 455- eng. pdf

Publisher's  Note  Springer  Nature  remains  neutral  with  regard  to 
jurisdictional claims in published maps and institutional affiliations.

Springer Nature or its licensor (e.g. a society or other partner) holds 
exclusive rights to this article under a publishing agreement with the 
author(s) or other rightsholder(s); author self-archiving of the accepted 
manuscript version of this article is solely governed by the terms of 
such publishing agreement and applicable law.

1 3
